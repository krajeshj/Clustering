---
title: "Clustering the  Wine data-set"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## GitHub Documents

This is an R Markdown format used for publishing markdown documents to GitHub. When you click the **Knit** button all R code chunks are run and a markdown file (.md) suitable for publishing to GitHub is generated.

## Including Code

You can include R code in the document as follows:


# Exercise 0: EXPLORE the data 
install these packages if you don't have them already
install.packages(c("cluster", "rattle","NbClust"))



Now load the data and look at the first few rows
```{r, echo=FALSE}
data(wine, package="rattle")
head(wine)
```
# Exercise 1: TRANSFORM  and LOAD : 
Remove the first column from the data and scale it using the scale() function
```{r, echo=FALSE}
df<-wine
df["Type"] <-NULL
str(df)
df <-scale(df)

df<-as.data.frame(df)
```
The type field is removed and we have also scaled the data
Scaling is data-mean/(Std. Deviation)

```{r}
head(df)
```
#### Now we'd like to cluster the data using K-Means. 
 How do we decide how many clusters to use if you don't know that already?
 We'll try two methods.

### Method 1: A plot of the total within-groups sums of squares 
plotted against against the  number of clusters in a K-means solution can be helpful. 
A bend in the graph can suggest the appropriate number of clusters. 
```
```{r}
wssplot <- function(data, nc=15, seed=1234){
	              wss <- (nrow(data)-1)*sum(apply(data,2,var))
               	      for (i in 2:nc){
		        set.seed(seed)
	                wss[i] <- sum(kmeans(data, centers=i)$withinss)}
	                
		      plot(1:nc, wss, type="b", xlab="Number of Clusters",
	                        ylab="Within groups sum of squares")
		      
}
```
#### Scree-plot
```{r}
wssplot(df)
```


# Exercise 2: Analyze the plot
#####   * How many clusters does this method suggest?
     Ans: 3 Clusters. The difference going from 1-3 is more than going from 3-4 on the Scree plot
#####   * Why does this method work? What's the intuition behind it?
     Ans: As the number of clusters increases the clusters are closer and more compact
#####   * Look at the code for wssplot() and figure out how it works
     Ans: For 1st iteration sum of variances of the df   is computed and
            multiplied by number of attributes : Entire cluster sum of squares is calulated
          For the next iteration i ... nc, the kmeans  with k = i is computed and its withinss is summed up
```{r}
print(" It looks like 3 -> 4 seems to be where the elbow is") 

print(" The drop in difference  is the most between 1 and 3 clusters")
```

### Method 2: Use the NbClust library, which runs many experiments
#### and gives a distribution of potential number of clusters.
``` {r}

library(NbClust)
set.seed(1234)
nc <- NbClust(df, min.nc=2, max.nc=15, method="kmeans")
```
```{r, echo=FALSE}
 
 barplot(table(nc$Best.n[1,]),
	          xlab="Numer of Clusters", ylab="Number of Criteria",
		            main="Number of Clusters Chosen by 26 Criteria")

print( "The NbClust method below also came up with number of clusters to be 3")
```
#  Exercise 3: NbClust method analysis
How many clusters does this method suggest?
```{r, echo=FALSE}
print(" Accordingto NbClust method it was 3")
```

# Exercise 4: Perform K-means Clustering with k=3
####Once you've picked the number of clusters, run k-means 
##### using this number of clusters. Output the result of calling kmeans()
##### into a variable fit.km
``` {r}
# fit.km <- kmeans( ... )
fit.km <- kmeans(df, 3, nstart=20)
# Now we want to evaluate how well this clustering does.
```

# Exercise 5: Compare kmeans fit_km with Wine types
####using the table() function, show how the clusters in fit.km$clusters
#### compares to the actual wine types in wine$Type. Would you consider this a good
#### clustering 
```{r}
table(fit.km$cluster, as.factor(wine$Type))
```

  Wine Types 1 and 3 are in perfect agreement
  Wine Type 2 is mostly  well clustered but not  in perfect agreement

# Exercise 6: Visualize
#### * Visualize these clusters using  function clusplot() from the cluster library
#### * Would you consider this a good clustering?
####     Ans: No, the inter cluster distance is small and the Diameters are large
clusplot( ... )
 
```{r results= "hide", warning=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE)
library("cluster")
```

```{r, echo = FALSE}
 clusplot (df,fit.km$cluster , diss = F, cor = T, stand = F, lines = 2, shade = F, color = F, labels = 0, plotchar = T, span = T)
```


 
 
 
 
 
 

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
